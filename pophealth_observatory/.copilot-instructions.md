applyTo: "pophealth_observatory/**/*.py", "tests/**/*.py"

# Python Code Instructions (PopHealth Observatory)

This document reflects the ACTUAL Python patterns in this repository. Follow these conventions to ensure consistency.

---

## TYPE HINTS
Type hints are used consistently and are required for all new function and method signatures.

- **Imports**: `from __future__ import annotations` is used to enable modern postponed evaluation. Common imports include `Callable`, `Sequence`, `Iterable` from `collections.abc`, and `Path` from `pathlib`.
- **Consistency**: All functions in the `pophealth_observatory` library include full type hints for parameters and return values.

**Examples from the codebase:**
```python
// filepath: pophealth_observatory/rag/pipeline.py
// ...existing code...
from collections.abc import Callable, Sequence
from pathlib import Path
// ...existing code...
def _load_snippets(path: Path) -> list[dict]:
// ...existing code...
def _format_prompt(question: str, snippets: Sequence[dict], max_chars: int = 3000) -> str:
// ...existing code...
class RAGPipeline:
// ...existing code...
    def generate(self, question: str, generator: GeneratorFn, top_k: int = 5) -> dict:
// ...existing code...
```
```python
// filepath: pophealth_observatory/pesticide_ingestion.py
// ...existing code...
from collections.abc import Iterable
// ...existing code...
def write_snippets(snippets: Iterable[Snippet], dest: Path) -> int:
// ...existing code...
```

---

## ML PIPELINE PATTERNS
The primary ML-like pattern is the custom Retrieval-Augmented Generation (RAG) pipeline, not scikit-learn pipelines.

- **Class**: The `RAGPipeline` class orchestrates the workflow.
- **Orchestration**: It follows a `prepare()` -> `retrieve()` -> `generate()` sequence.
  - `prepare()`: Loads data and builds or loads a vector embedding index.
  - `retrieve()`: Finds the most relevant snippets for a given question.
  - `generate()`: Assembles a prompt and passes it to an external generator function.
- **Preprocessing**: For text, preprocessing involves sentence segmentation (`segment_sentences`) and regex-based pattern indexing (`_index_analyte_patterns`) before ingestion into the pipeline.

**Example from the codebase:**
```python
// filepath: pophealth_observatory/rag/pipeline.py
// ...existing code...
class RAGPipeline:
// ...existing code...
    def generate(self, question: str, generator: GeneratorFn, top_k: int = 5) -> dict:
        snippets = self.retrieve(question, top_k=top_k)
        prompt = _format_prompt(question, snippets)
        answer = generator(question, snippets, prompt)
        return {"question": question, "answer": answer, "snippets": snippets, "prompt": prompt}

    def prepare(self) -> None:
        if not self._snippets:
            self.load_snippets()
        self.build_or_load_embeddings()
```

---

## DATA LOADING
Data is loaded from local files (JSON, JSONL) and remote URLs (.XPT). Loading from R via Parquet is a planned feature.

- **Path Construction**: `pathlib.Path` is used exclusively for all file system paths.
- **From JSONL**: Line-oriented JSON files are loaded by iterating through lines and parsing each with `json.loads()`. See `_load_snippets`.
- **From R (Planned)**: The established protocol is to use `pyarrow.parquet` to read files from a `shared_data/` directory. Do not use `reticulate`.

**Example from the codebase:**
```python
// filepath: pophealth_observatory/rag/pipeline.py
// ...existing code...
def _load_snippets(path: Path) -> list[dict]:
    data = []
    with path.open(encoding="utf-8") as fh:
        for line in fh:
            line = line.strip()
            if not line:
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError:  # pragma: no cover
                continue
    return data
```

---

## DATA STRUCTURES
`@dataclass` is the standard for creating structured data objects.

- **Usage**: The `Snippet` and `RAGConfig` classes are defined as dataclasses. This provides type safety and a clear structure over plain dictionaries.
- **Validation**: Validation is typically handled by the consuming functions, not within the dataclasses themselves.

**Example from the codebase:**
```python
// filepath: pophealth_observatory/pesticide_ingestion.py
// ...existing code...
@dataclass
class Snippet:
    cas_rn: str
    analyte_name: str
    parent_pesticide: str
    source_id: str
    source_path: str
    position: int
    sentence_window: list[str]

    def to_dict(self) -> dict[str, object]:
// ...existing code...
```

---

## FUNCTION STRUCTURE
Functions are kept small and focused, separating pure logic from side effects (I/O).

- **Pattern**: A common pattern is to have pure transformation functions (like `generate_snippets`) and thin I/O wrappers (like `write_snippets` and `ingest_text_file`).
- **Return Values**: Functions return structured objects (dataclasses, lists, dicts) rather than printing to stdout.

---

## DOCUMENTATION
Docstrings are present but style is mixed. New code should follow NumPy-style docstrings as specified in project guides.

**Example of desired format (from SETUP_GUIDE.md):**
```python
def train_model(X: pd.DataFrame, y: pd.Series) -> object:
    """
    Train a model.

    Parameters
    ----------
    X : pd.DataFrame
        Feature matrix.
    y : pd.Series
        Target variable.

    Returns
    -------
    model : object
        Trained model instance.
    """
    # ...
```

---

## ERROR HANDLING
Specific, built-in exceptions are preferred over generic ones.

- **Exceptions Raised**: `ValueError` for invalid parameters and `FileNotFoundError` for missing paths are common.
- **Network Failures**: For remote data fetching (e.g., NHANES .XPT files), the pattern is to log/print the error and return an empty `pandas.DataFrame` to allow downstream code to proceed gracefully.

---

## TESTING
The project uses `pytest` for testing, with tests located in the `tests` directory.

- **Structure**: Tests are organized by feature (e.g., `tests/test_rag_pipeline.py`).
- **Fixtures**: `tmp_path` is used for tests that involve file system I/O.
- **Assertions**: Simple `assert` statements are used to check for expected outcomes, such as list lengths, object types, and dictionary key presence.
- **Mocking**: For the RAG pipeline, a mock embedder is used to create deterministic tests for retrieval logic.

**Example from the codebase:**
```python
// filepath: tests/test_rag_pipeline.py
// ...existing code...
def _write_snippets(tmp_path: Path) -> Path:
    content = """
{"text": "Dimethylphosphate (DMP) levels decreased in 2022."}
{"text": "3-PBA remained stable across cohorts."}
{"text": "DEP findings were limited."}
{"text": "Unrelated nutritional note."}
""".strip()
    f = tmp_path / "snips.jsonl"
    f.write_text(content, encoding="utf-8")
    return f
```

---

## EXTERNAL LIBRARIES
The project relies on a standard scientific Python stack.

- **Data Libraries**: `pandas`, `numpy`
- **ML/RAG**: `sentence-transformers` (optional)
- **Visualization**: `matplotlib`, `seaborn`, `plotly`
- **Web/UI**: `requests`, `streamlit`
- **Tooling**: `pytest`, `ruff`, `black`, `mkdocs`

---

## EXPORT TO R
There is no code currently exporting data for R consumption.

- **Planned Protocol**: When implemented, data must be written as Parquet files to the `shared_data/` directory using `pyarrow`.
- **File Naming**: Files should be named `YYYY-MM-DD_<descriptor>.parquet`.
- **Prohibited Formats**: Do not use CSV or other formats for R/Python interchange.
