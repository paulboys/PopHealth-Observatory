{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1eedbfc",
   "metadata": {},
   "source": [
    "# NHANES 2021-2023 Demographics Data Link Finder\n",
    "\n",
    "This notebook programmatically locates the Demographics (DEMO) SAS transport (.XPT) data file link for the NHANES 2021-2023 cycle. It:\n",
    "\n",
    "- Fetches and validates the NHANES component listing page\n",
    "- Parses all anchors and scores them using pattern heuristics\n",
    "- Selects the best candidate DEMO .XPT file URL\n",
    "- Provides unit-style tests with mocked HTML\n",
    "- Optionally performs a live integration + file header inspection\n",
    "\n",
    "Cycle targeted: **2021-2023**  \n",
    "Component: **Demographics (DEMO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20447e4a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e7b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import typing as t\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# For lightweight inline testing without external pytest runner\n",
    "from types import SimpleNamespace\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "TARGET_URL = \"https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&Cycle=2021-2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a616e1e",
   "metadata": {},
   "source": [
    "## 2. Set Target URL and Matching Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9525c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured DEMO patterns and filters.\n"
     ]
    }
   ],
   "source": [
    "# Patterns & heuristics for identifying DEMO XPT link\n",
    "DEMO_PATTERNS = [\n",
    "    re.compile(r\"DEMO\", re.IGNORECASE),\n",
    "    re.compile(r\"Demographic\", re.IGNORECASE),\n",
    "]\n",
    "XPT_PATTERN = re.compile(r\"\\.xpt$\", re.IGNORECASE)\n",
    "\n",
    "# Candidate exclusion substrings (avoid documentation PDFs, etc.)\n",
    "EXCLUDE_SUBSTRINGS = [\".pdf\", \"DataDocs\", \"/tutorial\", \"/about\"]\n",
    "\n",
    "# Expected file naming shape (heuristic; may adapt)\n",
    "EXPECTED_PREFIX = \"DEMO\"\n",
    "\n",
    "print(\"Configured DEMO patterns and filters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee4247",
   "metadata": {},
   "source": [
    "## 3. HTTP Fetch with Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af815d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched bytes: 381776\n"
     ]
    }
   ],
   "source": [
    "def fetch_page(url: str, retries: int = 3, backoff: float = 1.0) -> requests.Response:\n",
    "    \"\"\"Fetch a URL with simple retry/backoff.\n",
    "    Raises requests.HTTPError if non-200 after retries.\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = SESSION.get(url, timeout=20)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                last_exc = requests.HTTPError(f\"Status {resp.status_code} on attempt {attempt}\")\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            last_exc = e\n",
    "        time.sleep(backoff * attempt)\n",
    "    raise last_exc if last_exc else RuntimeError(\"Unknown fetch failure\")\n",
    "\n",
    "# Quick smoke test (won't assert yet)\n",
    "_resp_preview = fetch_page(TARGET_URL)\n",
    "print(\"Fetched bytes:\", len(_resp_preview.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbdf6bb",
   "metadata": {},
   "source": [
    "## 4. Validate HTTP Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7918bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response validation passed.\n"
     ]
    }
   ],
   "source": [
    "def validate_html_response(resp: requests.Response) -> None:\n",
    "    assert resp.status_code == 200, f\"Unexpected status {resp.status_code}\"\n",
    "    ctype = resp.headers.get('Content-Type', '')\n",
    "    assert 'text' in ctype.lower() or 'html' in ctype.lower(), f\"Unexpected content type: {ctype}\"\n",
    "    assert resp.text.strip(), \"Empty body received\"\n",
    "\n",
    "validate_html_response(_resp_preview)\n",
    "print(\"Response validation passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76bbd9",
   "metadata": {},
   "source": [
    "## 5. Parse HTML and Collect Candidate Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d49cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 673 raw link candidates\n",
      "Top 5 by raw order:\n",
      " - #content | Skip directly to site content | score 0.0\n",
      " - https://www.cdc.gov | Centers for Disease Control and Preventi | score 0.0\n",
      " - https://www.cdc.gov/nchs/ | National Center for Health Statistics | score 0.0\n",
      " - #nav-group-about-nhanes | plus icon | score 0.0\n",
      " - https://www.cdc.gov/nchs/nhanes/nhanes-story.htm | Video: The NHANES Story | score 0.0\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class LinkCandidate:\n",
    "    href: str\n",
    "    text: str\n",
    "    title: str\n",
    "    score: float\n",
    "    raw: t.Dict[str, t.Any]\n",
    "\n",
    "\n",
    "def extract_link_candidates(html: str) -> t.List[LinkCandidate]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    anchors = soup.find_all('a')\n",
    "    candidates: t.List[LinkCandidate] = []\n",
    "    for a in anchors:\n",
    "        href = (a.get('href') or '').strip()\n",
    "        text = (a.get_text() or '').strip()\n",
    "        title = (a.get('title') or '').strip()\n",
    "        raw = {k: v for k, v in a.attrs.items()}\n",
    "        if not href:\n",
    "            continue\n",
    "        # Basic exclusion\n",
    "        if any(substr.lower() in href.lower() for substr in EXCLUDE_SUBSTRINGS):\n",
    "            continue\n",
    "        # Initial score\n",
    "        score = 0.0\n",
    "        if XPT_PATTERN.search(href):\n",
    "            score += 5\n",
    "        for pat in DEMO_PATTERNS:\n",
    "            if pat.search(href) or pat.search(text) or pat.search(title):\n",
    "                score += 3\n",
    "        if EXPECTED_PREFIX.lower() in href.lower():\n",
    "            score += 2\n",
    "        if EXPECTED_PREFIX.lower() in text.lower():\n",
    "            score += 1\n",
    "        if href.lower().endswith('.xpt') and 'demo' in href.lower():\n",
    "            score += 2\n",
    "        candidates.append(LinkCandidate(href=href, text=text, title=title, score=score, raw=raw))\n",
    "    return candidates\n",
    "\n",
    "cands = extract_link_candidates(_resp_preview.text)\n",
    "print(f\"Extracted {len(cands)} raw link candidates\")\n",
    "print(\"Top 5 by raw order:\")\n",
    "for c in cands[:5]:\n",
    "    print(\" -\", c.href[:80], \"|\", c.text[:40], \"| score\", c.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c1bb5",
   "metadata": {},
   "source": [
    "## 6. Filter for Demographic Data File Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4471b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked 1 candidate XPT links\n",
      "Score 13.0 | /Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt | text='DEMO_L Data [XPT - 2.5 MB]'\n"
     ]
    }
   ],
   "source": [
    "def rank_demographic_links(candidates: t.List[LinkCandidate]) -> t.List[LinkCandidate]:\n",
    "    # Only consider those with some base score & XPT extension\n",
    "    filtered = [c for c in candidates if c.href and XPT_PATTERN.search(c.href)]\n",
    "    # Additional penalty for suspiciously long query strings\n",
    "    for c in filtered:\n",
    "        if '?' in c.href:\n",
    "            c.score -= 1\n",
    "    # Sort descending by score then by shorter href length\n",
    "    return sorted(filtered, key=lambda c: (-c.score, len(c.href)))\n",
    "\n",
    "ranked = rank_demographic_links(cands)\n",
    "print(f\"Ranked {len(ranked)} candidate XPT links\")\n",
    "for r in ranked[:10]:\n",
    "    print(f\"Score {r.score:.1f} | {r.href} | text='{r.text[:30]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca133c00",
   "metadata": {},
   "source": [
    "## 7. Normalize and Build Absolute URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0987693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization & dedupe: 1\n",
      " * https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\n"
     ]
    }
   ],
   "source": [
    "def normalize_and_dedupe(ranked: t.List[LinkCandidate], page_url: str) -> t.List[LinkCandidate]:\n",
    "    seen = set()\n",
    "    out: t.List[LinkCandidate] = []\n",
    "    for c in ranked:\n",
    "        abs_url = urljoin(page_url, c.href)\n",
    "        if abs_url not in seen:\n",
    "            seen.add(abs_url)\n",
    "            out.append(LinkCandidate(href=abs_url, text=c.text, title=c.title, score=c.score, raw=c.raw))\n",
    "    return out\n",
    "\n",
    "normalized = normalize_and_dedupe(ranked, TARGET_URL)\n",
    "print(\"After normalization & dedupe:\", len(normalized))\n",
    "for c in normalized[:5]:\n",
    "    print(\" *\", c.href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eab3c0",
   "metadata": {},
   "source": [
    "## 8. Encapsulate Search Logic in Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2802cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidate URL: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\n"
     ]
    }
   ],
   "source": [
    "class DemographicFileNotFound(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def find_demographic_file_link(page_url: str = TARGET_URL) -> t.Dict[str, t.Any]:\n",
    "    resp = fetch_page(page_url)\n",
    "    validate_html_response(resp)\n",
    "    candidates = extract_link_candidates(resp.text)\n",
    "    ranked = rank_demographic_links(candidates)\n",
    "    normalized = normalize_and_dedupe(ranked, page_url)\n",
    "    if not normalized:\n",
    "        raise DemographicFileNotFound(\"No DEMO XPT link candidates found\")\n",
    "    best = normalized[0]\n",
    "    return {\n",
    "        \"url\": best.href,\n",
    "        \"anchor_text\": best.text,\n",
    "        \"title\": best.title,\n",
    "        \"score\": best.score,\n",
    "        \"all_candidates\": [c.href for c in normalized],\n",
    "    }\n",
    "\n",
    "# Try the function (non-fatal if fails)\n",
    "try:\n",
    "    result_preview = find_demographic_file_link()\n",
    "    print(\"Best candidate URL:\", result_preview[\"url\"])\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    print(\"Lookup failed (will test later):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb5fd4",
   "metadata": {},
   "source": [
    "## 9. Unit Tests with Mocked HTML (pytest-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8310f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit-style tests passed.\n"
     ]
    }
   ],
   "source": [
    "def _mock_response(html: str, status: int = 200) -> SimpleNamespace:\n",
    "    return SimpleNamespace(status_code=status, text=html, headers={\"Content-Type\": \"text/html\"}, content=html.encode())\n",
    "\n",
    "# Minimal mock test suite\n",
    "def test_single_valid_link():\n",
    "    html = '<a href=\"/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\">Demographics File</a>'\n",
    "    cands = extract_link_candidates(html)\n",
    "    ranked = rank_demographic_links(cands)\n",
    "    normalized = normalize_and_dedupe(ranked, TARGET_URL)\n",
    "    assert any(\"DEMO\" in c.href.upper() for c in normalized)\n",
    "\n",
    "\n",
    "def test_multiple_links_choose_demo():\n",
    "    html = '\\n'.join([\n",
    "        '<a href=\"/some/other/file.XPT\">Other File</a>',\n",
    "        '<a href=\"/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\">DEMO File</a>',\n",
    "        '<a href=\"/docs/manual.pdf\">Manual</a>'\n",
    "    ])\n",
    "    cands = extract_link_candidates(html)\n",
    "    ranked = rank_demographic_links(cands)\n",
    "    normalized = normalize_and_dedupe(ranked, TARGET_URL)\n",
    "    assert normalized[0].href.lower().endswith('.xpt')\n",
    "    assert 'demo' in normalized[0].href.lower()\n",
    "\n",
    "\n",
    "def test_no_links():\n",
    "    html = '<html><body><p>No anchors here</p></body></html>'\n",
    "    cands = extract_link_candidates(html)\n",
    "    ranked = rank_demographic_links(cands)\n",
    "    normalized = normalize_and_dedupe(ranked, TARGET_URL)\n",
    "    assert normalized == []\n",
    "\n",
    "# Run tests\n",
    "test_single_valid_link()\n",
    "test_multiple_links_choose_demo()\n",
    "test_no_links()\n",
    "print(\"Unit-style tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635acce2",
   "metadata": {},
   "source": [
    "## 10. Live Integration Test (Guarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46798ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live integration test skipped. Set RUN_LIVE_TEST=True to run.\n"
     ]
    }
   ],
   "source": [
    "RUN_LIVE_TEST = False  # Set to True to enable the live integration test\n",
    "\n",
    "if RUN_LIVE_TEST:\n",
    "    live_result = find_demographic_file_link()\n",
    "    print(\"Live link found:\", live_result[\"url\"])  \n",
    "    assert live_result[\"url\"].lower().endswith('.xpt')\n",
    "else:\n",
    "    print(\"Live integration test skipped. Set RUN_LIVE_TEST=True to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f458a6",
   "metadata": {},
   "source": [
    "## 11. Optional: Download and Inspect File Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74d1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file header from: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\n",
      "Loaded DataFrame shape: (11933, 27)\n",
      "Columns: ['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIAGENDR', 'RIDAGEYR', 'RIDAGEMN', 'RIDRETH1', 'RIDRETH3', 'RIDEXMON', 'RIDEXAGM', 'DMQMILIZ', 'DMDBORN4', 'DMDYRUSR', 'DMDEDUC2', 'DMDMARTZ']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "INSPECT_FILE = True  # Guard heavy download\n",
    "\n",
    "if INSPECT_FILE:\n",
    "    try:\n",
    "        meta = find_demographic_file_link()\n",
    "        file_url = meta['url']\n",
    "        print(\"Downloading file header from:\", file_url)\n",
    "        file_resp = SESSION.get(file_url, timeout=60)\n",
    "        file_resp.raise_for_status()\n",
    "        # Load into pandas\n",
    "        demo_df = pd.read_sas(io.BytesIO(file_resp.content), format='xport')\n",
    "        print(\"Loaded DataFrame shape:\", demo_df.shape)\n",
    "        print(\"Columns:\", list(demo_df.columns)[:15])\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        print(\"Failed to inspect file:\", e)\n",
    "else:\n",
    "    print(\"File inspection skipped. Set INSPECT_FILE=True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc808a7",
   "metadata": {},
   "source": [
    "## 12. Display Final Discovered Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2800d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved Demographics XPT URL:\n",
      " https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DEMO_L.xpt\n",
      "Anchor Text: DEMO_L Data [XPT - 2.5 MB]\n",
      "Score: 13.0\n",
      "Total candidate URLs considered: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    final_meta = find_demographic_file_link()\n",
    "    print(\"Resolved Demographics XPT URL:\\n\", final_meta['url'])\n",
    "    print(\"Anchor Text:\", final_meta['anchor_text'])\n",
    "    print(\"Score:\", final_meta['score'])\n",
    "    print(\"Total candidate URLs considered:\", len(final_meta['all_candidates']))\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    print(\"Final lookup failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0cc60",
   "metadata": {},
   "source": [
    "## 13. Multi-Component Page & File Link Discovery\n",
    "\n",
    "Goal: Starting from the Demographics component page (2021-2023 cycle), automatically locate and traverse the sibling component pages:\n",
    "- Demographics Data page\n",
    "- Examination Data page\n",
    "- Laboratory Data page\n",
    "\n",
    "Then extract every data file (.XPT) link listed on each of those pages.\n",
    "\n",
    "Approach:\n",
    "1. Parse the initial page for navigation or panel links referencing `Examination` and `Laboratory` (and confirm `Demographics`).\n",
    "2. Normalize and fetch each component page.\n",
    "3. Extract all `.xpt` links + associated anchor text, scoring & grouping.\n",
    "4. Produce a structured dictionary: `{component: [{file_url, anchor_text, score}]}`.\n",
    "\n",
    "Assumptions:\n",
    "- Component links contain keywords: `Examination`, `Laboratory`, `Demographics`.\n",
    "- Data file links end with `.XPT` or `.xpt`.\n",
    "- Relative URLs are resolved with `urljoin`. \n",
    "\n",
    "Fallback: If direct component page links not found, attempt heuristic construction (replacing `Component=Demographics` with target component name) and validate with HTTP 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9104432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "COMPONENT_KEYWORDS = {\n",
    "    'Demographics': re.compile(r'demograph', re.I),\n",
    "    'Examination': re.compile(r'examin', re.I),\n",
    "    'Laboratory': re.compile(r'laborat', re.I),\n",
    "}\n",
    "\n",
    "XPT_PATTERN = re.compile(r'\\.xpt$', re.I)\n",
    "\n",
    "\n",
    "def discover_component_pages(html: str, base_url: str) -> dict:\n",
    "    \"\"\"Discover component page URLs (Demographics, Examination, Laboratory) from a base component page HTML.\n",
    "    Strategy:\n",
    "      - Scan all anchors for keyword matches.\n",
    "      - Prefer links that contain 'Data' or 'data' in text or query string.\n",
    "      - Normalize & dedupe.\n",
    "      - Heuristic fallback: construct URLs by replacing Component=Demographics with other component names if missing.\n",
    "    Returns: {component: url or None}\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    found = {k: None for k in COMPONENT_KEYWORDS}\n",
    "    anchors = soup.find_all('a')\n",
    "\n",
    "    def qualify(a_text: str, href: str, comp: str) -> int:\n",
    "        score = 0\n",
    "        if not href:\n",
    "            return -1\n",
    "        if COMPONENT_KEYWORDS[comp].search(a_text or '') or COMPONENT_KEYWORDS[comp].search(href):\n",
    "            score += 5\n",
    "        if 'Component=' in href and comp in href:\n",
    "            score += 5\n",
    "        if 'Data' in a_text or 'data' in a_text:\n",
    "            score += 2\n",
    "        if 'Data' in href or 'data' in href:\n",
    "            score += 1\n",
    "        return score\n",
    "\n",
    "    candidates = {k: [] for k in COMPONENT_KEYWORDS}\n",
    "    for a in anchors:\n",
    "        text = (a.get_text() or '').strip()\n",
    "        href = a.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "        for comp in COMPONENT_KEYWORDS:\n",
    "            s = qualify(text, href, comp)\n",
    "            if s > 0:\n",
    "                full = urljoin(base_url, href)\n",
    "                candidates[comp].append((s, text, full))\n",
    "\n",
    "    for comp, rows in candidates.items():\n",
    "        if rows:\n",
    "            # pick highest score then shortest URL\n",
    "            rows.sort(key=lambda r: (-r[0], len(r[2])))\n",
    "            found[comp] = rows[0][2]\n",
    "\n",
    "    # Fallback heuristic construction\n",
    "    parsed = urlparse(base_url)\n",
    "    qs = parsed.query\n",
    "    if 'Component=Demographics' in qs:\n",
    "        for comp in COMPONENT_KEYWORDS:\n",
    "            if found[comp] is None:\n",
    "                heuristic = base_url.replace('Component=Demographics', f'Component={comp}')\n",
    "                # quick HEAD/GET validation\n",
    "                try:\n",
    "                    r = requests.get(heuristic, timeout=10)\n",
    "                    if r.status_code == 200 and COMPONENT_KEYWORDS[comp].search(r.text):\n",
    "                        found[comp] = heuristic\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return found\n",
    "\n",
    "\n",
    "def extract_xpt_links(page_url: str) -> list:\n",
    "    \"\"\"Fetch a component page and return list of dicts with XPT file link details.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(page_url, timeout=20)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    out = []\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "        if XPT_PATTERN.search(href):\n",
    "            full = urljoin(page_url, href)\n",
    "            text = (a.get_text() or '').strip()\n",
    "            score = 0\n",
    "            if 'DEMO' in href.upper():\n",
    "                score += 3\n",
    "            if 'LAB' in href.upper():\n",
    "                score += 2\n",
    "            if 'EXAM' in href.upper() or 'BMX' in href.upper():\n",
    "                score += 2\n",
    "            if 'Questionnaire' in text:\n",
    "                score += 1\n",
    "            out.append({\n",
    "                'file_url': full,\n",
    "                'anchor_text': text,\n",
    "                'raw_href': href,\n",
    "                'score': score\n",
    "            })\n",
    "    # Deduplicate by file_url keeping max score\n",
    "    dedup = {}\n",
    "    for rec in out:\n",
    "        u = rec['file_url']\n",
    "        if u not in dedup or rec['score'] > dedup[u]['score']:\n",
    "            dedup[u] = rec\n",
    "    return list(dedup.values())\n",
    "\n",
    "\n",
    "def aggregate_component_files(component_pages: dict) -> dict:\n",
    "    result = {}\n",
    "    for comp, url in component_pages.items():\n",
    "        if not url:\n",
    "            result[comp] = []\n",
    "            continue\n",
    "        files = extract_xpt_links(url)\n",
    "        # annotate with component\n",
    "        for f in files:\n",
    "            f['component'] = comp\n",
    "        result[comp] = files\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b289c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching base demographics page ...\n",
      "Status: 200\n",
      "Discovered component pages:\n",
      "  Demographics: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics\n",
      "  Examination: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination\n",
      "  Laboratory: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory\n",
      "\n",
      "Extracting XPT file links per component ...\n",
      "Demographics: 12 XPT link(s)\n",
      "Examination: 182 XPT link(s)\n",
      "Laboratory: 749 XPT link(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component</th>\n",
       "      <th>file_url</th>\n",
       "      <th>anchor_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_D Data [XPT - 3.4 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_E Data [XPT - 3.3 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_C Data [XPT - 3.4 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_B Data [XPT - 3.1 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1...</td>\n",
       "      <td>DEMO Data [XPT - 11 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_F Data [XPT - 3.5 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_G Data [XPT - 3.6 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_H Data [XPT - 3.7 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_I Data [XPT - 3.6 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_J Data [XPT - 3.3 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>P_DEMO Data [XPT - 3.4 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>DEMO_L Data [XPT - 2.5 MB]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_D Data [XPT - 2.1 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1...</td>\n",
       "      <td>BMX Data [XPT - 2.7 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_E Data [XPT - 1.7 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_C Data [XPT - 2.4 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_B Data [XPT - 2.1 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_F Data [XPT - 1.8 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_H Data [XPT - 2 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_G Data [XPT - 1.9 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_I Data [XPT - 1.9 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_J Data [XPT - 1.4 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>P_BMX Data [XPT - 2.4 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>BMX_L Data [XPT - 1.5 MB]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Examination</td>\n",
       "      <td>https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...</td>\n",
       "      <td>ARX_F Data [XPT - 510.5 KB]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       component                                           file_url  \\\n",
       "0   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "1   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "2   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "3   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "4   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1...   \n",
       "5   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "6   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "7   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "8   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "9   Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "10  Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "11  Demographics  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "66   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "67   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/1...   \n",
       "68   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "69   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "70   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "71   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "72   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "73   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "74   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "75   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "76   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "77   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "12   Examination  https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2...   \n",
       "\n",
       "                    anchor_text  score  \n",
       "0    DEMO_D Data [XPT - 3.4 MB]      3  \n",
       "1    DEMO_E Data [XPT - 3.3 MB]      3  \n",
       "2    DEMO_C Data [XPT - 3.4 MB]      3  \n",
       "3    DEMO_B Data [XPT - 3.1 MB]      3  \n",
       "4       DEMO Data [XPT - 11 MB]      3  \n",
       "5    DEMO_F Data [XPT - 3.5 MB]      3  \n",
       "6    DEMO_G Data [XPT - 3.6 MB]      3  \n",
       "7    DEMO_H Data [XPT - 3.7 MB]      3  \n",
       "8    DEMO_I Data [XPT - 3.6 MB]      3  \n",
       "9    DEMO_J Data [XPT - 3.3 MB]      3  \n",
       "10   P_DEMO Data [XPT - 3.4 MB]      3  \n",
       "11   DEMO_L Data [XPT - 2.5 MB]      3  \n",
       "66    BMX_D Data [XPT - 2.1 MB]      2  \n",
       "67      BMX Data [XPT - 2.7 MB]      2  \n",
       "68    BMX_E Data [XPT - 1.7 MB]      2  \n",
       "69    BMX_C Data [XPT - 2.4 MB]      2  \n",
       "70    BMX_B Data [XPT - 2.1 MB]      2  \n",
       "71    BMX_F Data [XPT - 1.8 MB]      2  \n",
       "72      BMX_H Data [XPT - 2 MB]      2  \n",
       "73    BMX_G Data [XPT - 1.9 MB]      2  \n",
       "74    BMX_I Data [XPT - 1.9 MB]      2  \n",
       "75    BMX_J Data [XPT - 1.4 MB]      2  \n",
       "76    P_BMX Data [XPT - 2.4 MB]      2  \n",
       "77    BMX_L Data [XPT - 1.5 MB]      2  \n",
       "12  ARX_F Data [XPT - 510.5 KB]      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_DEMO_URL = \"https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&Cycle=2021-2023\"\n",
    "\n",
    "print(\"Fetching base demographics page ...\")\n",
    "resp = requests.get(BASE_DEMO_URL, timeout=20)\n",
    "print(f\"Status: {resp.status_code}\")\n",
    "component_pages = {}\n",
    "if resp.status_code == 200:\n",
    "    component_pages = discover_component_pages(resp.text, BASE_DEMO_URL)\n",
    "else:\n",
    "    print(\"Failed to fetch base page; aborting component discovery.\")\n",
    "\n",
    "print(\"Discovered component pages:\")\n",
    "for k, v in component_pages.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nExtracting XPT file links per component ...\")\n",
    "component_files = aggregate_component_files(component_pages)\n",
    "\n",
    "# Summarize counts\n",
    "for comp, files in component_files.items():\n",
    "    print(f\"{comp}: {len(files)} XPT link(s)\")\n",
    "\n",
    "# Flatten for DataFrame if pandas available\n",
    "try:\n",
    "    import pandas as pd\n",
    "    flat_rows = []\n",
    "    for comp, files in component_files.items():\n",
    "        for rec in files:\n",
    "            flat_rows.append({\n",
    "                'component': comp,\n",
    "                'file_url': rec['file_url'],\n",
    "                'anchor_text': rec['anchor_text'],\n",
    "                'score': rec['score']\n",
    "            })\n",
    "    if flat_rows:\n",
    "        df_files = pd.DataFrame(flat_rows).sort_values(['component','score'], ascending=[True, False])\n",
    "        display(df_files.head(25))\n",
    "    else:\n",
    "        print(\"No file links discovered.\")\n",
    "except Exception as e:\n",
    "    print(\"Pandas not available or error creating DataFrame:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7087edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest written to component_files_manifest_2021_2023.json (components: 3; total files: 943)\n",
      "Components in manifest: ['Demographics', 'Examination', 'Laboratory']\n"
     ]
    }
   ],
   "source": [
    "# Optional: export manifest to JSON for reuse\n",
    "import json, time\n",
    "manifest = {\n",
    "    'cycle': '2021-2023',\n",
    "    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'component_pages': component_pages,\n",
    "    'files': component_files,\n",
    "}\n",
    "\n",
    "output_path = 'component_files_manifest_2021_2023.json'\n",
    "try:\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    print(f\"Manifest written to {output_path} (components: {len(component_pages)}; total files: {sum(len(v) for v in component_files.values())})\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to write manifest:\", e)\n",
    "\n",
    "# Quick peek at keys\n",
    "print('Components in manifest:', list(component_files.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pophealth_observatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
